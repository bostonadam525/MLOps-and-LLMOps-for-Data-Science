{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "251ee242-98ac-4a3a-8918-6a3ada97d574",
   "metadata": {},
   "source": [
    "# Sagemaker AI - Deploy Hugging Face LLMs to Endpoint\n",
    "* Notebook by Adam Lang\n",
    "* Date: 3/26/2025\n",
    "\n",
    "# Overview\n",
    "* This is a simple notebook of how to deploy a hugging face transformer and a decoder LLM Falcon to a Sagemaker endpoint and run inference on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c353b77e-3387-468d-8020-b388cc5be631",
   "metadata": {},
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9909a4cd-8b48-4739-82e5-5bcbf949ead9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U sagemaker "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13c90b9-fe43-4349-a33f-8de4bcc96638",
   "metadata": {},
   "source": [
    "# Sagemaker Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40eeb70c-ca33-4f51-bc4a-a937e7907028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker \n",
    "import boto3\n",
    "\n",
    "## init sagemaker session\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "## sagemaker bucket session -- used for uploading data, models, logs\n",
    "## sagemaker automatically creates this bucket if it doesn't exist\n",
    "sagemaker_session_bucket=None \n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set default bucket if bucket name not given \n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "## sagemaker Role management\n",
    "try:\n",
    "    role=sagemaker.get_execution_role()\n",
    "except ValueError: \n",
    "    iam=boto3.client(\"iam\") \n",
    "    role=iam.get_role(RoleName=\"sagemaker_execution_role\")['Role']['Arn']\n",
    "\n",
    "\n",
    "## init session with default bucket\n",
    "session=sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "## print arn role & region_name\n",
    "print(f\"Sagemaker role arn: {role}\")\n",
    "print(f\"Sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b12c2bf-3f8b-4242-9f10-8cef5dd955ed",
   "metadata": {},
   "source": [
    "# Load a Hugging Face Model\n",
    "* Model loaded: `distilbert/distilbert-base-uncased-distilled-squad`\n",
    "* Model card: https://huggingface.co/distilbert/distilbert-base-uncased-distilled-squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a23ddb54-a638-4898-bd8f-a3eea9998fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  \n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7d22bda-38f0-42cf-b185-bb244c85a316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.0+cu121\n",
      "Transformers version: 4.48.0\n"
     ]
    }
   ],
   "source": [
    "## check versions of torch and transformers\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2155d1c9-a0ec-4ca0-b7c7-f33099d55c86",
   "metadata": {},
   "source": [
    "Downgrade transformers and pytorch versions to work with hugging face version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d742d3e9-ff1e-406c-9040-630255f16a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install transformers==4.48.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c4b6f04-a2d6-410e-8ac3-86229c3a8d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "pip install torch==2.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af5d7276-c0ea-4233-80f8-786b80518611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.0+cu121\n",
      "Transformers version: 4.48.0\n"
     ]
    }
   ],
   "source": [
    "## Again check versions of torch and transformers\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cdb0e4e8-3b17-43ab-96f6-9145aa1c35e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.11\n"
     ]
    }
   ],
   "source": [
    "## check python version\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2c856c5-edf9-47a8-a0e9-3462f3beec7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "## HF hub model config\n",
    "hf_hub = {\n",
    "    \"HF_MODEL_ID\": 'distilbert/distilbert-base-uncased-distilled-squad', ## HF model id\n",
    "    \"HF_TASK\": \"question-answering\" ## input prediction task \n",
    "} \n",
    "## create Hugging Face Model Class \n",
    "huggingface_model = HuggingFaceModel(\n",
    "    env=hf_hub,                    ## HF hub model config\n",
    "    role=role,                     ## IAM role permissions in AWS Sagemaker\n",
    "    transformers_version=\"4.48.0\", ## transformers versio using\n",
    "    pytorch_version=\"2.3.0\", ## pytorch version using\n",
    "    py_version=\"py311\",     ## python version to use \n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09f4e3d-50f5-4ca2-a561-2c058480bcb1",
   "metadata": {},
   "source": [
    "# Deploy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68440fb5-861a-40e3-b2f2-281f9bfc867a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[03/26/25 19:18:59] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating model with name:                                              <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#4094\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4094</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         huggingface-pytorch-inference-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-03-26-19-18-59-838                  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[03/26/25 19:18:59]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating model with name:                                              \u001b]8;id=301492;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=392759;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#4094\u001b\\\u001b[2m4094\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         huggingface-pytorch-inference-\u001b[1;36m2025\u001b[0m-03-26-19-18-59-838                  \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[03/26/25 19:19:00] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating endpoint-config with name                                     <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#5937\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5937</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         huggingface-pytorch-inference-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-03-26-19-19-00-426                  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[03/26/25 19:19:00]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating endpoint-config with name                                     \u001b]8;id=388802;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=795070;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#5937\u001b\\\u001b[2m5937\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         huggingface-pytorch-inference-\u001b[1;36m2025\u001b[0m-03-26-19-19-00-426                  \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating endpoint with name                                            <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#4759\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4759</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         huggingface-pytorch-inference-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-03-26-19-19-00-426                  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating endpoint with name                                            \u001b]8;id=441130;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=538525;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#4759\u001b\\\u001b[2m4759\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         huggingface-pytorch-inference-\u001b[1;36m2025\u001b[0m-03-26-19-19-00-426                  \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------!"
     ]
    }
   ],
   "source": [
    "## deploy model for Sagemaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\", ## \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc83bac-9b8d-4cc6-b595-7e1fdc490113",
   "metadata": {},
   "source": [
    "# Inference with Deployed Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72b36531-190f-4015-9900-9419af9cdc9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.998674750328064, 'start': 71, 'end': 80, 'answer': 'sagemaker'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## test request #1  \n",
    "data = {\n",
    "    \"inputs\": {\n",
    "        \"question\": \"What is used for inference?\",\n",
    "        \"context\": \"My name is Joe and I work in a button factory. This model is used with sagemaker for inference.\"\n",
    "    }\n",
    "}\n",
    "## make request \n",
    "predictor.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "748945bd-8cf1-4ea5-82f4-d72c1ebe9892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.2989235520362854, 'start': 26, 'end': 38, 'answer': 'NFL football'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## test request #2 \n",
    "data = {\n",
    "    \"inputs\": {\n",
    "        \"question\": \"What does Tom like?\",\n",
    "        \"context\": \"My name is Tom and I play NFL football for the New England Patriots.\"\n",
    "    }\n",
    "}\n",
    "## make request \n",
    "predictor.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd82dc8-7567-4503-a50f-5d85fe4b99ba",
   "metadata": {},
   "source": [
    "# Delete Endpoint\n",
    "* docs: https://huggingface.co/docs/sagemaker/en/inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "614b2fac-6a36-47ce-a05b-972a97179d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[03/26/25 19:33:38] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Deleting endpoint configuration with name:                             <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#4913\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4913</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         huggingface-pytorch-inference-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-03-26-19-19-00-426                  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[03/26/25 19:33:38]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Deleting endpoint configuration with name:                             \u001b]8;id=873779;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=647758;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#4913\u001b\\\u001b[2m4913\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         huggingface-pytorch-inference-\u001b[1;36m2025\u001b[0m-03-26-19-19-00-426                  \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[03/26/25 19:33:39] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Deleting endpoint with name:                                           <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#4903\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4903</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         huggingface-pytorch-inference-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-03-26-19-19-00-426                  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[03/26/25 19:33:39]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Deleting endpoint with name:                                           \u001b]8;id=555097;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=129007;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#4903\u001b\\\u001b[2m4903\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         huggingface-pytorch-inference-\u001b[1;36m2025\u001b[0m-03-26-19-19-00-426                  \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# delete endpoint\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d550eb-84c6-4d3f-97c4-88d2789f7f0a",
   "metadata": {},
   "source": [
    "# LLM Endpoint Deployment\n",
    "* This is an example of a more advanced sagemaker endpoint deployment than the simple example above since we will deploy a large language model not a small encoder model as we did above. \n",
    "* The steps are mostly the same with a few exceptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f758fbe0-8986-4874-b851-ec6b67ccb6b0",
   "metadata": {},
   "source": [
    "## Upgrade Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d699108-ec65-4091-ad98-79d3157728dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U sagemaker "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee942fd3-9cb6-4887-abfd-a591de37f8b4",
   "metadata": {},
   "source": [
    "## Sagemaker Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577729af-cbdd-4848-bc58-32cb5891e3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker \n",
    "import boto3\n",
    "\n",
    "## init sagemaker session\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "## sagemaker bucket session -- used for uploading data, models, logs\n",
    "## sagemaker automatically creates this bucket if it doesn't exist\n",
    "sagemaker_session_bucket=None \n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set default bucket if bucket name not given \n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "## sagemaker Role management\n",
    "try:\n",
    "    role=sagemaker.get_execution_role()\n",
    "except ValueError: \n",
    "    iam=boto3.client(\"iam\") \n",
    "    role=iam.get_role(RoleName=\"sagemaker_execution_role\")['Role']['Arn']\n",
    "\n",
    "\n",
    "## init session with default bucket\n",
    "session=sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "## print arn role & region_name\n",
    "print(f\"Sagemaker role arn: {role}\")\n",
    "print(f\"Sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a00985-d72c-4c81-9bb0-d760ed4d44cd",
   "metadata": {},
   "source": [
    "## Advanced Model Loading\n",
    "* Compared to deploying a regular hugging face model, we first need to get a container URI and give it to the `HuggingFaceModel` class with an `image_uri` that points to the image.\n",
    "* To obtain the Hugging Face LLM Deep Learning Container in AWS Sagemaker we use the `get_huggingface_llm_image_uri` method via the Sagemaker SDK.\n",
    "* This method will allow us to get the URI for the Hugging Face LLM Deep Learning Container of choice for the specific session, region, version, and backend.\n",
    "* Essentially we place the ENTIRE LLM in a Container and then call the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c2814e-9873-486d-ac8e-a2b5cef10d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "## get LLM image URI \n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "    \"huggingface\",\n",
    "    version=\"0.8.2\",\n",
    ")\n",
    "\n",
    "## get ECR image URI \n",
    "print(f\"LLM image URI: {llm_image}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5ad4f6-868a-488f-898d-d75160ac1120",
   "metadata": {},
   "source": [
    "# Deploy an LLM in AWS Sagemaker\n",
    "* To deploy a large language model such as `Falcon-40B-Instruct`, we first need to create the HuggingFaceModel class and define an endpoint config via the `hf_model_id` and the `instance_type`.\n",
    "* For endpoint inference we will try to use the `ml.g5.2xlarge`.\n",
    "* This is the model we will deploy: `tiiuae/falcon-40b-instruct`\n",
    "  * model card: https://huggingface.co/tiiuae/falcon-40b-instruct\n",
    "  * This is a causal decoder only model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a94ade57-04ec-48b1-85a4-67bba3d58dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "## setup sagemaker config \n",
    "instance_type = \"ml.g5.2xlarge\" \n",
    "number_of_gpu = 4 \n",
    "\n",
    "# TGI config\n",
    "config = {\n",
    "    \"HF_MODEL_ID\": \"tiiuae/falcon-40b-instruct\", ## HF model id checkpoint \n",
    "    \"SM_NUM_GPUS\": json.dumps(number_of_gpu), ## number of GPU used per replica \n",
    "    \"MAX_INPUT_LENGTH\": json.dumps(1024), ## max len input to LLM \n",
    "    \"MAX_TOTAL_TOKENS\": json.dumps(2048) ## max len of generation tokens (including input)\n",
    "    # \"HF_MODEL_QUANTIZE\": \"bitsandbytes\", ## uncomment to quantize LLM \n",
    "\n",
    "}\n",
    "\n",
    "## now create class HuggingFaceModel \n",
    "llm_model = HuggingFaceModel(\n",
    "    role=role, ## IAM role \n",
    "    image_uri=llm_image, ## image uri from above \n",
    "    env=config, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaa24e7-384d-4768-856d-5bf226809858",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Deploy model to an endpoint \n",
    "llm = llm_model.deploy(\n",
    "    initial_instance_count=1, \n",
    "    instance_type=instance_type, \n",
    "    # volume_size=400, ## if using instance with local SSD storage, volume_size must be None, e.g. p4 but not p3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4e9e85-4b88-468c-bb6a-4790ee733b13",
   "metadata": {},
   "source": [
    "# Inference with Deployed LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cb43c7-370a-48be-8717-0e4deb1ffe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define payload for LLM \n",
    "prompt = \"\"\"You are a very helpful assistant named Falcon and you know a lot about AWS.\n",
    "\n",
    "User: Can you tell me 3 important points about AWS Sagemaker?\n",
    "Falcon:\"\"\"\n",
    "\n",
    "## LLM hyperparameters \n",
    "payload = { \n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": { \n",
    "        \"do_sample\": True, \n",
    "        \"top_p\": 0.8,\n",
    "        \"temperature\": 0.5,\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"repetition_penalty\": 1.04, \n",
    "        \"stop\": [\"\\nUser:\", \"<|endoftext|>\", \"</s>\"]  # Corrected this line\n",
    "    }\n",
    "}\n",
    "\n",
    "## send request to endpoint\n",
    "response = llm.predict(payload)\n",
    "\n",
    "# response is a dictionary with a 'generated_text' key\n",
    "if isinstance(response, dict) and 'generated_text' in response:\n",
    "    print(f\"Result: {response['generated_text']}\")\n",
    "elif isinstance(response, list):\n",
    "    for seq in response:\n",
    "        if isinstance(seq, dict) and 'generated_text' in seq:\n",
    "            print(f\"Result: {seq['generated_text']}\")\n",
    "        else:\n",
    "            print(f\"Unexpected sequence format: {seq}\")\n",
    "else:\n",
    "    print(f\"Unexpected response format: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d18d27-056b-4166-b121-76d4619dab38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
